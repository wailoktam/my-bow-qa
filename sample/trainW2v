# !/usr/bin/env python
# -*- coding:utf-8 -*-
import gensim
import os
import re, pprint
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
import site
import numpy
import unicodedata
import re
import string
import codecs
import kanjinums
from pyknp import Juman
from py4j.java_gateway import JavaGateway
from operator import is_not
from functools import partial

gateway = JavaGateway()

from xml.etree import ElementTree as etree

from gensim.models import word2vec

def myNormalize(inputStr):
    KuroTokenizer = gateway.jvm.org.atilika.kuromoji.Tokenizer
    tokenizer = KuroTokenizer.builder().build();
#    print ("inputStr"+inputStr+"\n")
    result = tokenizer.tokenize(inputStr)
    normalized = []
    for token in result:
        print ("token"+"["+token.getSurfaceForm()+"]"+"\n")
        try:
            normalizedToken = kanjinums.kanji2num(token.getSurfaceForm())
        except KeyError:
            normalizedToken = token.getSurfaceForm()
        try:
            normalized.append(str(normalizedToken))
        except Exception:
            normalized.append(normalizedToken)
#    normalized = unicodedata.normalize('NFKC',unicode(normalized))
    return(normalized)

def pp(obj):
    pp = pprint.PrettyPrinter(indent=4, width=160)
    str = pp.pformat(obj)
    return re.sub(r"\\u([0-9a-f]{4})", lambda x: unichr(int("0x"+x.group(1), 16)), str)




if __name__ == '__main__':
#    window = sys.argv[1]
    sentences = word2vec.Text8Corpus('/mnt/Works/wailoktam/trainingSentences/segmentedSentenceList.txt')
    model = word2vec.Word2Vec(sentences)
    model.save('/home/wailoktam/model')
    xml = etree.parse("/home/wailoktam/qa/mylab/input/questions/qa-sampleDocRetrievedBySect.xml")
    questions = xml.findall(".//question")
    labels = numpy.array([])
    lengthsQ = numpy.array([])
    lengthsA = numpy.array([])
    for question in questions:
        questionText = question.find(".//text").text
        print ("questionText"+"["+ "/".join(myNormalize(questionText))+"]"+"\n")
        print ("length of questionText"+str(len(myNormalize(questionText)))+"\n")
        lengthsQ = numpy.append(lengthsQ,len(myNormalize(questionText.strip())))
#        print ("numby array containing lengthS of questionText"+"\n")
#        print (lengthsQ)
        answers = map(lambda a: a.text, question.findall(".//answer"))
        answers = filter(partial(is_not, None), answers)
        docs = question.findall(".//doc")
        for doc in docs:
            doc_txt = doc.find(".//dtext").text
            for sentence in doc_txt.split("。"):
                answerFoundFlag = False
                normalizedSentence = myNormalize(sentence.strip())
                numpy.append(lengthsA,len(normalizedSentence))
                for answer in map(lambda a: myNormalize(a), answers):
                    joinedAnswer = "".join(answer).strip()
                    joinedSentence = "".join(normalizedSentence).strip()
                    if joinedAnswer in joinedSentence:
                        answerFoundFlag = True
                if answerFoundFlag:
                    numpy.append(labels,1)
                else:
                    numpy.append(labels,0)
#    print ("length of questionText2"+"/".join(lengthsQ)+"\n")
    print ("numby array containing lengthS of questionText"+"\n")
    print (lengthsQ)
    print ("maximum q"+numpy.amax(lengthsQ))
    print ("minimum q"+numpy.amin(lengthsQ))
    print ("maximum a"+numpy.amax(lengthsA))
    print ("minimum a"+numpy.amin(lengthsA))

    # TODO: question のタイプによって、スコアの計算方法を変える
    # ここではとりあえず precision/recall を計算
#        question_id = question.get("id")
#        answers = filter(partial(is_not, None), answers)

#           doc_txt = doc.find(".//dtext").text


#    responses = filter(partial(is_not, None), responses)
    #if len(responses) == 0: print ("no response %s" % (question_id))



#    num_correct_answers = float(len(set(map(lambda a: myNormalize(a), answers)) & set(map(lambda a: myNormalize(a), responses))))
#    precision = float(num_correct_answers / len(responses))
#    recall = float(num_correct_answers / len(answers))
##    if num_correct_answers:
 #       print ("found in title: questionId %s docTitle %s" % (question_id,doc_title.strip()))
#        sum_found = sum_found + 1
#        by_type_sum_found[questionType] = by_type_sum_found[questionType]+1
#    for answer in answers:
#        found_once = 0



    savedModel = word2vec.Word2Vec.load('/home/wailoktam/model')
#model = Word2Vec.load_word2vec_format('/home/wailoktam/model', unicode_errors='ignore')
    out = savedModel.most_similar(positive=[u'冒険'])
    vector = model[u'冒険']
    print(pp(out[1]))
    print (vector)