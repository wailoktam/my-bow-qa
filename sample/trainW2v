# !/usr/bin/env python
# -*- coding:utf-8 -*-
import gensim
import os
import re, pprint
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
import site
import numpy
import unicodedata
import re
import string
import codecs
import kanjinums
import pandas
import matplotlib
matplotlib.use("Agg")
import pylab
from matplotlib import pyplot
from pyknp import Juman
from py4j.java_gateway import JavaGateway
from operator import is_not
from functools import partial

gateway = JavaGateway()

from xml.etree import ElementTree as etree

from gensim.models import word2vec

def myNormalize(inputStr):
    KuroTokenizer = gateway.jvm.org.atilika.kuromoji.Tokenizer
    tokenizer = KuroTokenizer.builder().build();
#    print ("inputStr"+inputStr+"\n")
    result = tokenizer.tokenize(inputStr)
    normalized = []
    for token in result:
        print ("token"+"["+token.getSurfaceForm()+"]"+"\n")
        try:
            normalizedToken = kanjinums.kanji2num(token.getSurfaceForm())
        except KeyError:
            normalizedToken = token.getSurfaceForm()
        try:
            normalized.append(str(normalizedToken))
        except Exception:
            normalized.append(normalizedToken)
#    normalized = unicodedata.normalize('NFKC',unicode(normalized))
    return(normalized)

def pp(obj):
    pp = pprint.PrettyPrinter(indent=4, width=160)
    str = pp.pformat(obj)
    return re.sub(r"\\u([0-9a-f]{4})", lambda x: unichr(int("0x"+x.group(1), 16)), str)




if __name__ == '__main__':
#    window = sys.argv[1]
    wikiOnePPPath = "/home/wailoktam/qa/mylab/onePagePerFile"
    sentenceList = []
    corpusFileName = '/mnt/Works/wailoktam/trainingSentences/segmentedS'
    corpusFile  = codecs.open(corpusFileName, 'w', 'utf-8')
    for subPath in os.listdir(wikiOnePPPath):
        fullPath = os.path.join(wikiOnePPPath,subPath)
        print ("path"+fullPath+"\n")
        if os.path.isfile(fullPath):
            parsedXml = etree.parse(fullPath)
            print(parsedXml.getroot().tag)
            print(parsedXml.getroot().text)
            pageText = parsedXml.find("./id").text
            print ("pageText"+pageText+"\n")
            paras = parsedXml.findall(".//para")
            for para in paras:
                print ("para:"+para.text+"\n")
                for sent in para.text.strip().split("。"):
                    print ("sent",sent+"\n")
                    corpusFile.writeln(sent)
    corpusFile.close()
    sentences = word2vec.Text8Corpus(corpusFileName)
    model = word2vec.Word2Vec(sentences)
    model.save('/home/wailoktam/model')



    savedModel = word2vec.Word2Vec.load('/home/wailoktam/model')
#model = Word2Vec.load_word2vec_format('/home/wailoktam/model', unicode_errors='ignore')
    out = savedModel.most_similar(positive=[u'冒険'])
    vector = savedModel[u'冒険']
    print(pp(out[1]))
    print (vector)